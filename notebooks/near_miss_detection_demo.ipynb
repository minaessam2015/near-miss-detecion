{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# Near-Miss Incident Detection Pipeline\n",
    "**Tatweer AI/ML Take-Home Challenge — Option 1: Computer Vision**\n",
    "\n",
    "This notebook implements a full end-to-end system for detecting near-miss traffic incidents in a video clip.  \n",
    "All core logic lives in `src/` Python modules; this notebook orchestrates and visualises results.\n",
    "\n",
    "**Pipeline overview:**\n",
    "1. Download & inspect the video\n",
    "2. Detect objects with YOLOv8n (CPU)\n",
    "3. Track objects with a Centroid Tracker\n",
    "4. Detect near-miss events (proximity + TTC + risk scoring)\n",
    "5. Generate annotated video + dashboard visualisations + HTML report\n",
    "6. **Bonus A** — False-positive filter comparison\n",
    "7. **Bonus B** — Real-time performance benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup\n",
    "Install dependencies and configure paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Install dependencies (safe to re-run) ─────────────────────────────────\n",
    "import subprocess, sys\n",
    "\n",
    "pkgs = [\n",
    "    'opencv-python',\n",
    "    'ultralytics',\n",
    "    'yt-dlp',\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'pandas',\n",
    "    'scipy',\n",
    "    'Pillow',\n",
    "    'onnxruntime',\n",
    "]\n",
    "\n",
    "# CPU-only torch to keep install fast on Colab\n",
    "torch_cmd = [\n",
    "    sys.executable, '-m', 'pip', 'install', '-q',\n",
    "    'torch', 'torchvision',\n",
    "    '--index-url', 'https://download.pytorch.org/whl/cpu'\n",
    "]\n",
    "\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs, check=True)\n",
    "subprocess.run(torch_cmd, check=True)\n",
    "\n",
    "print('All packages installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-paths",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root : /Users/mina.essam/Downloads/tatweer\n",
      "Source dir   : /Users/mina.essam/Downloads/tatweer/src\n",
      "Output dir   : /Users/mina.essam/Downloads/tatweer/outputs\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# ── Resolve project root (works on Colab and locally) ─────────────────────\n",
    "NOTEBOOK_DIR = os.path.abspath('.')\n",
    "# If running from notebooks/, go up one level\n",
    "PROJECT_ROOT = (\n",
    "    os.path.dirname(NOTEBOOK_DIR)\n",
    "    if os.path.basename(NOTEBOOK_DIR) == 'notebooks'\n",
    "    else NOTEBOOK_DIR\n",
    ")\n",
    "SRC_DIR = os.path.join(PROJECT_ROOT, 'src')\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs')\n",
    "\n",
    "for d in [SRC_DIR, DATA_DIR, OUTPUT_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "if SRC_DIR not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f'Project root : {PROJECT_ROOT}')\n",
    "print(f'Source dir   : {SRC_DIR}')\n",
    "print(f'Output dir   : {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Image as IPImage\n",
    "\n",
    "from src.video_utils import (\n",
    "    download_video, get_video_metadata, sample_frames,\n",
    "    frame_generator, display_sample_frames,\n",
    ")\n",
    "from src.detector import ObjectDetector, draw_detections\n",
    "from src.tracker import create_tracker\n",
    "from src.near_miss import NearMissDetectorV11\n",
    "from src.visualizer import (\n",
    "    annotate_frame, create_annotated_video,\n",
    "    plot_timeline, plot_risk_distribution,\n",
    "    plot_heatmap, plot_frequency,\n",
    "    generate_html_report,\n",
    ")\n",
    "\n",
    "print('Imports OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-video",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Video Download & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-download",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=YF_DzoTDO-0\n",
      "[youtube] YF_DzoTDO-0: Downloading webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] No supported JavaScript runtime could be found. Only deno is enabled by default; to use another runtime add  --js-runtimes RUNTIME[:PATH]  to your command/config. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] YF_DzoTDO-0: Downloading android vr player API JSON\n",
      "[info] YF_DzoTDO-0: Downloading 1 format(s): 398\n",
      "[download] Destination: /Users/mina.essam/Downloads/tatweer/data/traffic_video_3.mp4\n",
      "[download] 100% of    4.05MiB in 00:00:22 at 180.79KiB/s \n",
      "\n",
      "── Video Metadata ────────────────────────\n",
      "  fps            : 30.0\n",
      "  frame_count    : 888\n",
      "  width          : 1280\n",
      "  height         : 720\n",
      "  duration_sec   : 00:29.60 (29.6s)\n"
     ]
    }
   ],
   "source": [
    "VIDEO_URL = 'https://www.youtube.com/watch?v=r86kxxU-LUY'\n",
    "VIDEO_URL = 'https://www.youtube.com/watch?v=YF_DzoTDO-0'\n",
    "VIDEO_PATH = os.path.join(DATA_DIR, 'traffic_video_3.mp4')\n",
    "\n",
    "if not os.path.exists(VIDEO_PATH):\n",
    "    print('Downloading video...')\n",
    "    download_video(VIDEO_URL, VIDEO_PATH, quality='720')\n",
    "else:\n",
    "    print(f'Video already exists: {VIDEO_PATH}')\n",
    "\n",
    "meta = get_video_metadata(VIDEO_PATH)\n",
    "print('\\n── Video Metadata ────────────────────────')\n",
    "for k, v in meta.items():\n",
    "    if k == 'duration_sec':\n",
    "        mm = int(v // 60); ss = v % 60\n",
    "        print(f'  {k:15s}: {mm:02d}:{ss:05.2f} ({v:.1f}s)')\n",
    "    else:\n",
    "        print(f'  {k:15s}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sample-frames",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 5 evenly-spaced frames to visually inspect the video\n",
    "frames = sample_frames(VIDEO_PATH, n=5)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for ax, (idx, frame) in zip(axes, frames):\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(rgb)\n",
    "    ts = idx / meta['fps']\n",
    "    ax.set_title(f'Frame {idx}\\n{ts:.1f}s', fontsize=9)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Frames', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'sample_frames.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Sample frames saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-detection",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Object Detection Preview\n",
    "Run YOLOv8n on 10 sampled frames and display annotated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-detector-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detector — downloads yolov8n.pt on first run (~6 MB)\n",
    "FRAME_STRIDE = 2        # process every 2nd frame\n",
    "INPUT_SIZE   = 640      # YOLO input resolution\n",
    "CONF_THRESH  = 0.40\n",
    "\n",
    "detector = ObjectDetector(\n",
    "    model_name='yolov8n.pt',\n",
    "    conf=CONF_THRESH,\n",
    "    input_size=INPUT_SIZE,\n",
    ")\n",
    "print('Detector ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-detection-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on 10 sample frames, display annotated grid\n",
    "preview_frames = sample_frames(VIDEO_PATH, n=10)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(22, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "detection_counts = []\n",
    "for ax, (idx, frame) in zip(axes, preview_frames):\n",
    "    dets = detector.detect(frame)\n",
    "    detection_counts.append(len(dets))\n",
    "    annotated = draw_detections(frame, dets)\n",
    "    rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(rgb)\n",
    "    ts = idx / meta['fps']\n",
    "    ax.set_title(f'Frame {idx} ({ts:.1f}s)\\n{len(dets)} detections', fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Detection Preview — YOLOv8n', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'detection_preview.png'), dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nAvg inference time : {detector.avg_inference_ms:.1f} ms/frame')\n",
    "print(f'Avg detections     : {sum(detection_counts)/len(detection_counts):.1f} per frame')\n",
    "detector.reset_timing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-full-pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Full Pipeline Run\n",
    "Process the entire video: detect → track → near-miss detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pipeline-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configuration\n",
    "EFFECTIVE_FPS   = meta['fps'] / FRAME_STRIDE   # FPS after stride\n",
    "PROXIMITY_PX    = 120          # centroid distance threshold (px)\n",
    "TTC_THRESHOLD   = 2.5          # time-to-collision threshold (seconds)\n",
    "DEBOUNCE_FRAMES = 30           # min frames between same-pair events\n",
    "MAX_DISTANCE    = 200          # max tracking match distance (px)\n",
    "MAX_DISAPPEARED = 15           # frames before deregistering a track\n",
    "\n",
    "print(f'Effective FPS      : {EFFECTIVE_FPS:.2f}')\n",
    "print(f'Proximity threshold: {PROXIMITY_PX} px')\n",
    "print(f'TTC threshold      : {TTC_THRESHOLD} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-run-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = create_tracker(\n",
    "    'centroid',\n",
    "    max_disappeared=MAX_DISAPPEARED,\n",
    "    max_distance=MAX_DISTANCE,\n",
    "    min_hits=2,\n",
    ")\n",
    "nm_detector = NearMissDetectorV11(\n",
    "    proximity_px=PROXIMITY_PX,\n",
    "    ttc_threshold=TTC_THRESHOLD,\n",
    "    fps=EFFECTIVE_FPS,\n",
    "    debounce_frames=DEBOUNCE_FRAMES,\n",
    "    filters_enabled=True,\n",
    "\n",
    "    clearance_scale=1.1,\n",
    "\n",
    ")\n",
    "\n",
    "# frame_data[frame_idx] = (tracked_objects_snapshot, active_pairs)\n",
    "frame_data  = {}\n",
    "peak_active = 0\n",
    "\n",
    "t_start = time.perf_counter()\n",
    "\n",
    "for frame_idx, frame in frame_generator(VIDEO_PATH, stride=FRAME_STRIDE):\n",
    "    # 1. Detect\n",
    "    dets = detector.detect(frame)\n",
    "\n",
    "    # 2. Track\n",
    "    tracked = tracker.update(dets, frame_idx)\n",
    "    peak_active = max(peak_active, len(tracked))\n",
    "\n",
    "    # 3. Near-miss\n",
    "    events = nm_detector.process_frame(frame_idx, tracked)\n",
    "    active_pairs = nm_detector.active_pairs(frame_idx)\n",
    "\n",
    "    # Save snapshot for annotation pass\n",
    "    frame_data[frame_idx] = (dict(tracked), list(active_pairs))\n",
    "\n",
    "t_end = time.perf_counter()\n",
    "total_time = t_end - t_start\n",
    "\n",
    "events_df = nm_detector.get_events_dataframe()\n",
    "summary   = nm_detector.summary()\n",
    "\n",
    "print(f'\\n── Pipeline Complete ──────────────────────────────')\n",
    "print(f'  Total processing time : {total_time/60:.1f} min ({total_time:.0f}s)')\n",
    "print(f'  Avg inference         : {detector.avg_inference_ms:.1f} ms/frame')\n",
    "print(f'  Frames processed      : {len(frame_data)}')\n",
    "print(f'  Unique object IDs     : {tracker.total_ids_assigned}')\n",
    "print(f'  Peak simultaneous     : {peak_active} objects')\n",
    "print(f'  Near-miss events      : {summary[\"total\"]} total')\n",
    "print(f'    High  : {summary[\"high\"]}')\n",
    "print(f'    Medium: {summary[\"medium\"]}')\n",
    "print(f'    Low   : {summary[\"low\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-results",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Event Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-events-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if events_df.empty:\n",
    "    print('No near-miss events detected. Try lowering PROXIMITY_PX or TTC_THRESHOLD.')\n",
    "else:\n",
    "    print(f'Total events: {len(events_df)}')\n",
    "    display(events_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-top-events",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not events_df.empty:\n",
    "    print('── Top 5 Highest-Risk Events ──────────────')\n",
    "    top5 = events_df.nlargest(5, 'risk_score')[\n",
    "        ['timestamp_sec', 'class_1', 'class_2',\n",
    "         'distance_px', 'ttc_sec', 'risk_score', 'risk_level']\n",
    "    ]\n",
    "    display(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-viz",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Dashboard Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all 4 charts\n",
    "path_timeline = os.path.join(OUTPUT_DIR, 'timeline.png')\n",
    "path_pie      = os.path.join(OUTPUT_DIR, 'risk_distribution.png')\n",
    "path_heatmap  = os.path.join(OUTPUT_DIR, 'heatmap.png')\n",
    "path_freq     = os.path.join(OUTPUT_DIR, 'frequency.png')\n",
    "\n",
    "# First frame for heatmap background\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "_, first_frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "plot_timeline(events_df, meta['duration_sec'], path_timeline)\n",
    "plot_risk_distribution(events_df, path_pie)\n",
    "plot_heatmap(tracker.all_trajectories, first_frame, path_heatmap)\n",
    "plot_frequency(events_df, meta['duration_sec'], path_freq, bin_sec=15)\n",
    "\n",
    "# Display all inline\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "chart_paths = [\n",
    "    (path_timeline, 'Event Timeline'),\n",
    "    (path_pie, 'Risk Distribution'),\n",
    "    (path_heatmap, 'Activity Heatmap'),\n",
    "    (path_freq, 'Frequency Over Time'),\n",
    "]\n",
    "for ax, (path, title) in zip(axes.flatten(), chart_paths):\n",
    "    if os.path.exists(path):\n",
    "        img = mpimg.imread(path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Near-Miss Detection Dashboard', fontsize=15, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'dashboard.png'), dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('All charts saved to outputs/.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-video-out",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Annotated Output Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-annotated-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_OUT = os.path.join(OUTPUT_DIR, 'annotated_video.mp4')\n",
    "\n",
    "print('Writing annotated video (this may take a few minutes)...')\n",
    "create_annotated_video(\n",
    "    video_path=VIDEO_PATH,\n",
    "    output_path=VIDEO_OUT,\n",
    "    frame_data=frame_data,\n",
    "    fps=EFFECTIVE_FPS,\n",
    ")\n",
    "\n",
    "size_mb = os.path.getsize(VIDEO_OUT) / 1e6\n",
    "print(f'Done. File size: {size_mb:.1f} MB')\n",
    "\n",
    "# On Colab, provide a download link\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print('Downloading annotated_video.mp4 to your machine...')\n",
    "    files.download(VIDEO_OUT)\n",
    "except ImportError:\n",
    "    print(f'Output saved locally at: {VIDEO_OUT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-report",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. HTML Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-html-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_PATH = os.path.join(OUTPUT_DIR, 'report.html')\n",
    "\n",
    "# Compute tracker stats for the report\n",
    "all_trajs = tracker.all_trajectories\n",
    "avg_track_len = (\n",
    "    round(sum(len(t) for t in all_trajs.values()) / len(all_trajs), 1)\n",
    "    if all_trajs else 0\n",
    ")\n",
    "tracker_stats = {\n",
    "    'total_ids': tracker.total_ids_assigned,\n",
    "    'avg_track_length': avg_track_len,\n",
    "    'peak_active': peak_active,\n",
    "}\n",
    "\n",
    "img_paths = {\n",
    "    'timeline': path_timeline,\n",
    "    'risk_distribution': path_pie,\n",
    "    'heatmap': path_heatmap,\n",
    "    'frequency': path_freq,\n",
    "}\n",
    "\n",
    "generate_html_report(\n",
    "    metadata=meta,\n",
    "    events_df=events_df,\n",
    "    tracker_stats=tracker_stats,\n",
    "    img_paths=img_paths,\n",
    "    output_path=REPORT_PATH,\n",
    ")\n",
    "\n",
    "# On Colab, provide a download link\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(REPORT_PATH)\n",
    "except ImportError:\n",
    "    print(f'Report saved locally at: {REPORT_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-fp-filter",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Bonus A — False-Positive Filter Comparison\n",
    "Re-run near-miss detection **without** the FP filters and compare event counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fp-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run with filters DISABLED (reuse the already-computed tracker.all_trajectories)\n",
    "nm_unfiltered = NearMissDetectorV11(\n",
    "    proximity_px=PROXIMITY_PX,\n",
    "    ttc_threshold=TTC_THRESHOLD,\n",
    "    fps=EFFECTIVE_FPS,\n",
    "    debounce_frames=DEBOUNCE_FRAMES,\n",
    "    filters_enabled=False,   # <-- key difference\n",
    "    clearance_scale=1.1,\n",
    ")\n",
    "\n",
    "for frame_idx, (tracked_snap, _) in frame_data.items():\n",
    "    nm_unfiltered.process_frame(frame_idx, tracked_snap)\n",
    "\n",
    "unfiltered_df = nm_unfiltered.get_events_dataframe()\n",
    "filtered_df   = events_df  # already computed with filters=True\n",
    "\n",
    "# Comparison table\n",
    "def summarise(df, label):\n",
    "    if df.empty:\n",
    "        return {'Config': label, 'Total': 0, 'High': 0, 'Medium': 0, 'Low': 0}\n",
    "    counts = df['risk_level'].value_counts().to_dict()\n",
    "    return {\n",
    "        'Config': label,\n",
    "        'Total': len(df),\n",
    "        'High': counts.get('High', 0),\n",
    "        'Medium': counts.get('Medium', 0),\n",
    "        'Low': counts.get('Low', 0),\n",
    "    }\n",
    "\n",
    "cmp_df = pd.DataFrame([\n",
    "    summarise(unfiltered_df, 'Without FP Filters'),\n",
    "    summarise(filtered_df,   'With FP Filters'),\n",
    "])\n",
    "cmp_df['Reduction %'] = [\n",
    "    0.0,\n",
    "    round((1 - len(filtered_df) / max(len(unfiltered_df), 1)) * 100, 1),\n",
    "]\n",
    "\n",
    "print('── False-Positive Filter Comparison ──────────')\n",
    "display(cmp_df)\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "x = range(len(cmp_df))\n",
    "ax.bar([i - 0.2 for i in x], cmp_df['High'],   0.18, label='High',   color='#e74c3c')\n",
    "ax.bar([i       for i in x], cmp_df['Medium'], 0.18, label='Medium', color='#e67e22')\n",
    "ax.bar([i + 0.2 for i in x], cmp_df['Low'],    0.18, label='Low',    color='#f1c40f')\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(cmp_df['Config'])\n",
    "ax.set_ylabel('Event Count')\n",
    "ax.set_title('Effect of False-Positive Filters')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'fp_filter_comparison.png'), dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Comparison chart saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-fp-manual",
   "metadata": {},
   "source": [
    "### Manual Event Review\n",
    "Display the 5 events that were **filtered out** (in unfiltered but not in filtered)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-fp-manual-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events present in unfiltered but not in filtered (by frame_index + id pair)\n",
    "def event_key(row):\n",
    "    return (row['frame_index'], min(row['object_id_1'], row['object_id_2']),\n",
    "            max(row['object_id_1'], row['object_id_2']))\n",
    "\n",
    "if not unfiltered_df.empty and not filtered_df.empty:\n",
    "    unf_keys = set(unfiltered_df.apply(event_key, axis=1))\n",
    "    fil_keys  = set(filtered_df.apply(event_key, axis=1))\n",
    "    removed_keys = unf_keys - fil_keys\n",
    "\n",
    "    removed_events = unfiltered_df[\n",
    "        unfiltered_df.apply(event_key, axis=1).isin(removed_keys)\n",
    "    ].head(5)\n",
    "\n",
    "    if removed_events.empty:\n",
    "        print('No events were filtered out — filters had no effect on this video.')\n",
    "    else:\n",
    "        print(f'Showing {len(removed_events)} filtered-out event(s):')\n",
    "        display(removed_events[['frame_index','timestamp_sec','class_1','class_2',\n",
    "                                 'distance_px','ttc_sec','risk_score','risk_level']])\n",
    "elif unfiltered_df.empty:\n",
    "    print('Unfiltered detector found no events either.')\n",
    "else:\n",
    "    print('All events were retained by the filter.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-benchmark",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Bonus B — Performance Benchmark\n",
    "Compare FPS across different stride/resolution/backend configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.optimizer import benchmark_pipeline, recommend_config, export_to_onnx\n",
    "\n",
    "# Export ONNX model for benchmarking\n",
    "ONNX_PATH = os.path.join(OUTPUT_DIR, 'yolov8n.onnx')\n",
    "if not os.path.exists(ONNX_PATH):\n",
    "    print('Exporting YOLOv8n to ONNX...')\n",
    "    exported = export_to_onnx(detector.model, ONNX_PATH)\n",
    "    # ultralytics saves to the model's location; copy to OUTPUT_DIR if needed\n",
    "    import shutil\n",
    "    if not os.path.exists(ONNX_PATH) and os.path.exists('yolov8n.onnx'):\n",
    "        shutil.move('yolov8n.onnx', ONNX_PATH)\n",
    "else:\n",
    "    print(f'ONNX model already exists: {ONNX_PATH}')\n",
    "\n",
    "# Benchmark configs\n",
    "configs = [\n",
    "    {'stride': 1, 'input_size': 640, 'backend': 'pytorch'},\n",
    "    {'stride': 2, 'input_size': 640, 'backend': 'pytorch'},\n",
    "    {'stride': 4, 'input_size': 640, 'backend': 'pytorch'},\n",
    "    {'stride': 2, 'input_size': 320, 'backend': 'pytorch'},\n",
    "    {'stride': 2, 'input_size': 640, 'backend': 'onnx', 'onnx_path': ONNX_PATH},\n",
    "    {'stride': 2, 'input_size': 320, 'backend': 'onnx', 'onnx_path': ONNX_PATH},\n",
    "]\n",
    "\n",
    "bench_df = benchmark_pipeline(VIDEO_PATH, configs, n_frames=60)\n",
    "print('\\n── Benchmark Results ─────────────────────────────')\n",
    "display(bench_df)\n",
    "\n",
    "best = recommend_config(bench_df)\n",
    "print(f'\\nRecommended config: {best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-benchmark-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise benchmark results\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "colors = ['#e74c3c' if fps >= 15 else '#3498db' for fps in bench_df['fps']]\n",
    "bars = ax.barh(bench_df['config_name'], bench_df['fps'], color=colors)\n",
    "ax.axvline(x=15, color='red', linestyle='--', linewidth=1.5, label='15 FPS target')\n",
    "ax.set_xlabel('Throughput (FPS)')\n",
    "ax.set_title('Pipeline Benchmark — CPU Throughput')\n",
    "ax.legend()\n",
    "\n",
    "for bar, fps in zip(bars, bench_df['fps']):\n",
    "    ax.text(bar.get_width() + 0.2, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{fps:.1f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'benchmark.png'), dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Benchmark chart saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-benchmark-analysis",
   "metadata": {},
   "source": [
    "### Benchmark Analysis\n",
    "\n",
    "**Key trade-offs observed:**\n",
    "\n",
    "| Optimization | FPS Impact | Quality Impact |\n",
    "|---|---|---|\n",
    "| Stride 1 → 4 | ~4× faster | May miss short-duration events |\n",
    "| Resolution 640 → 320 | ~2× faster | Lower detection accuracy for small/distant objects |\n",
    "| PyTorch → ONNX | ~1.2–1.5× faster | Same accuracy (identical weights) |\n",
    "\n",
    "**Production recommendation:**  \n",
    "For a real deployment with fixed cameras, `stride=2 + input_size=320 + ONNX` offers the best  \n",
    "throughput/quality balance. For archival analysis where time is not critical, `stride=1 + 640px` \n",
    "gives the most accurate near-miss detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-outputs-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Output Files Summary\n",
    "\n",
    "| File | Description |\n",
    "|---|---|\n",
    "| `outputs/annotated_video.mp4` | Full video with bounding boxes and risk overlays |\n",
    "| `outputs/report.html` | Self-contained HTML report with embedded charts |\n",
    "| `outputs/timeline.png` | Near-miss events on the video timeline |\n",
    "| `outputs/risk_distribution.png` | Pie chart of High / Medium / Low events |\n",
    "| `outputs/heatmap.png` | Object activity density heatmap |\n",
    "| `outputs/frequency.png` | Event frequency per 15-second interval |\n",
    "| `outputs/fp_filter_comparison.png` | Before/after false-positive filtering |\n",
    "| `outputs/benchmark.png` | CPU throughput across configurations |\n",
    "| `outputs/dashboard.png` | Combined 4-chart dashboard |\n",
    "| `outputs/detection_preview.png` | Sample detection frames grid |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tatweer_vision_task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}